\section{Introduction} \label{sec:intro}
Frequently, complex systems are described in terms of stochastic processes, as the
underlying deterministic process is too complex to be modeled exactly or as the
process is indeed random. It is not always the random process itself that is of 
interest, but a derived quantity. For example, the distribution of waiting times until the
process reaches a certain state. 

In the field of cognitive psychology, random processes are frequently used to
describe each processing stage in a chain of stages that leads to a response. The state of
each random process itself is usually not measurable but the total response time of all processing 
stages involved. Although each processing stage may be modeled as a random process, the waiting-time
of a single stage is just a random variable%
\footnote{This requires that the random process is \emph{reset} for each sample. This is usually 
the case if the waiting time of the processing stage is determined by the time a random process
starting at a specified state reaches a certain end state. Then, the waiting times of the processing
stage are independent samples from a simple random variable with some distribution.}%
and the complete system is therefore a 
system of dependent random variables \cite[e.g., the EZ-Reader model,][]{Reichle2003}.

\pkg{StochBB} is able to describe and analyze complex systems of dependent random variables
by combining simple ones (representing single stages with a known waiting-time distribution) to
a complex system. For example, consider the following independent random variables
\begin{equation}
  X_1 \sim \Gamma(10, 100)\,, X_2 \sim \Gamma(20, 50)\text{ and } X_3 \sim \text{Exp}(0.01)\,,\nonumber
\end{equation}
which are described completely by their distribution. In terms of \emph{processing stages}, this means that the time, the
processing stage $X_1$ needs to complete is gamma-distributed with shape $k=10$ and scale
$\theta=100$. Analogously, the stages $X_2$ and $X_3$ are defined by their own waiting-time 
distribution.

From these basic building blocks, a more complex system can be assembled by
combining them. Using the example above, one may assume sequential processing. That is, a chain of
the stages $X_1,\dots,X_3$. This simple chain then describes the successive
processing of information entering the first stage described by $X_1$.
Once the first processing stage finishes, its result gets forwarded to the second stage described
by $X_2$ and finally to the last stage described by $X_3$. The waiting time of the
complete chain is again a random variable that is the sum of all random variables,
or expressed mathematically
\begin{equation}
 Y = X_1 + X_2 + X_3\,. \nonumber
\end{equation}

\pkg{SochBB} determines the probability density function (PDF) or cumulative probability function
(CDF) of the waiting-time distribution of $Y$ analytically (as far as
possible) or resorts to a numeric method if the analytic approach fails 
\cite[see][for examples of numerical approximations outperforming the
stochastic simulation approach]{Thomas2012, Thomas2013}. Moreover it provides an
efficient and correct sampler for the system of random variables.

In the following section, I outline the representation of random variables and the
reductions performed on a network. In section \ref{sec:arch}, I describe the
architecture of the framework followed by a in-detail description of the
graphical user interface (GUI) application that allows to assemble and analyze
networks of dependent random variables easily. In section \ref{sec:pod}, I
showcase the usage of the GUI application for an analysis of a simple response
latency model. Finally, in section \ref{sec:data}, I describe how parameters of
the distributions of random variables can be estimated from data.


\section{Representation and reductions of random variables}
Continuing the example above, please note that the sum of random variables commutes. Hence the
random variable $Y$ remains the same if defined as $Y = X_1 + X_3 + X_2$ instead of 
$Y = X_1 + X_2 + X_3$. Moreover, the 
distribution of the sum of $X_1$ and $X_3$ can be determined analytically as 
$X' = (X_1+X_3)\sim\Gamma(11,100)$. Hence the random variable $Y$ can now be expressed as 
$Y = X_2 + X'$, and only a single numeric convolution is necessary to obtain the PDF of the
random variable $Y$. \pkg{SochBB} implements several reductions, exploiting mathematical
identities of random variables and their distributions. To this end, it allows to obtain their PDFs and 
CDFs efficiently. In this section, all basic building blocks that are currently implemented and their 
reductions are presented and discussed in some detail.

\subsection{Affine transformations of random variables}
An affine transformation of the random variable $X$ has the form $Y = a\,X+b$, where $a\neq 0$
and $b$ are real values. Although affine transformations of random variables are not frequently
used directly in a system of random variables, they may appear as a result of other reductions
of the system. The PDF and CDF of the random variable $Y$ defined above, are then 
\begin{equation}
 f_Y(y) = \frac{1}{a}f_X\left(\frac{y-b}{a}\right)\text{ and }
 F_Y(y) = F_X\left(\frac{y-b}{a}\right)\,. \nonumber
\end{equation}

Of course, an affine transformation of an affine transformed random variable $X$ is also a simple 
affine transformation of the random variable. Hence the following reduction is implemented
\begin{equation}
 c\,(a\,X+b)+d \longrightarrow (a\,c)\,X+(c\,b+d)\,.\nonumber
\end{equation}

\subsection{Sums of random variables}
Sums of random variables have been introduced briefly above and may represent a chain of processing
stages being triggered sequentially. The sum itself is a derived random variable that depends on all
mutually independent variables being summed up. The PDF of the sum $Y$ is then the convolution of all
PDFs of the summed variables. That is
\begin{align}
 Y &= \sum_{i=0}^NX_i\text{ where } X_i \sim f_i(x)\text{ mutually independent} \nonumber \\
 Y &\sim f_1(x) \ast \cdots \ast f_N(x)\,. \nonumber
\end{align}

The direct numerical convolution of the underlying distributions can be computationally expensive if the
number of PDFs is large. Assuming that all distributions are well supported on a common interval, however, 
allows for a fast numerical convolution by means of FFT convolution \cite[e.g., ][]{Press2007}. 
This requires that the grid is chosen such that all densities being convoluted as well as the
result are well supported on the chosen interval and the grid must be fine enough to
capture the details of all distributions. 

Like any numerical approach, the FFT convolution is only an approximation. Hence \pkg{SochBB}
tries to perform convolutions analytically before resorting to the numerical approach. 

First, all sums of random variables are flattened. That is, 
\begin{equation}
 \begin{array}{l}
  Y_1 = X_1 + X_2\\
  Y_2 = Y_1 + X_3 
 \end{array} \longrightarrow 
 \begin{array}{l}
  Y_1 = X_1 + X_2\\
  Y_2 = X_1 + X_2 + X_3 
 \end{array}\,, \nonumber
\end{equation}
and all common terms are collected
\begin{equation}
 X_1+X_2+X_1 \longrightarrow 2\,X_1+X_2\,. \nonumber
\end{equation}

Then, the distribution of the sum is derived. Here the following reductions are performed.
\begin{align}
 \delta(x-x_0)\ast f(x) &\longrightarrow f(x-x_0)\,, \nonumber \\
 \phi(x; \mu_1, \sigma_1)\ast \phi(x; \mu_2, \sigma_2) &\longrightarrow 
   \phi(x; \mu_1+\mu_2, \sqrt{\sigma_1^2+\sigma_2^2}) \nonumber\,, \\
 \Gamma(x; k_1, \theta)\ast \Gamma(x; k_2, \theta) &\longrightarrow 
   \Gamma(x; k_1+k_2, \theta)\,, \nonumber
\end{align}
where $\delta(\cdot-x_0)$ is the delta distribution located at $x_0$, $\phi(\cdot; \mu, \sigma)$ 
the normal distribution with mean $\mu$ and standard deviation $\sigma$ and 
$\Gamma(\cdot; k, \theta)$ the gamma distribution with shape $k$ and scale $\theta$.

\subsection{Minimum and maximum of random variables}
The minimum or maximum of some given random variables, e.g. $Y=\min\{X_1,X_2\}$ or 
$Y=\max\{X_1,X_2\}$ are themselves random variables. These derived random variables may
express the waiting time of a processing stage that consists of several independent processing stages being
performed in parallel (in contrast to sequential processing described by sums of random variables). 
If the complete system finishes once the first of the parallel processing stages finishes, the 
resulting waiting time is the minimum of the underlying random variables. If the system
finishes once all underlying processing stages are finished, the resulting waiting time is the maximum.	 

If the two independent random variables $X_1$ and $X_2$ are distributed according to the (cumulative) distribution
functions $F_1(x)$ and $F_2(x)$, the maximum of these two random variables $Y=\max\{X_1,X_2\}$
is distributed according to the probability function $F_Y(y)=F_1(y)\cdot F_2(y)$. Consequently, its PDF
is $f_Y(y)=f_1(y)\cdot F_2(y) + f_2(y)\cdot F_1(y)$, where $f_1(\cdot)$ and $f_2(\cdot)$ are the PDFs
of the random variables $X_1$ and $X_2$. Likewise, the CDF and PDF of the minimum can be expressed as
$F_Y(y)=1-(1-F_1(y))\cdot(1-F_2(y))$ and therefore $f_Y(y) = f_1(y)(1-F_2(y)) + f_2(y)(1-F_1(y))$.

For applying these equations to derive the CDFs and PDFs of the minimum and maximum random variables, the 
underlying random variables must be independent. To achieve independence where possible, the following 
reductions are performed first. In a first step, the maximum and minimum structures were flattened, for example
\begin{equation}
 \begin{array}{l}
  Y_1 = \max\{X_1,X_2\}\\
  Y_2 = \max\{Y_1,X_3\}
 \end{array} \longrightarrow
 \begin{array}{l}
  Y_1 = \max\{X_1,X_2\}\\
  Y_2 = \max\{X_1,X_2,X_3\}
 \end{array}\,, \nonumber
\end{equation}
then, possible common terms are collected like
\begin{equation}
 \max\{X_1 + X_2, X_3 + X_2\} \longrightarrow \max\{X_1,X_3\}+X_2\,. \nonumber
\end{equation}

The latter transformation does not \emph{ensure} independence of random variables, but 
decreases the complexity of setting-up a complex system of random variables by 
resolving simple-structured dependencies between random variables analytically.

\subsection{Mixture of random variables}
A mixture is the weighted sum of random variables
\begin{equation}
 Y = \frac{w_1X_1+\cdots+w_NX_N}{w_1+\cdots+w_N}\,, \nonumber
\end{equation}
where $w_i$ are positive weights. The result $Y$ is also a random variable with the PDF
\begin{equation}
 f_Y(y) = \frac{w_1f_1(x)+\cdots+w_Nf_N(x)}{w_1+\cdots+w_N}\,,\nonumber
\end{equation}
where $f_i(\cdot)$ is the PDF of the i-th random variable $X_i$. The CDF of the
mixture is obtained analogously. A mixture can be used to describe a random 
path-selection in a system of processing stages. 

\subsection{Conditional random variables}
The \emph{conditional} random variable selects one of two random variables (e.g. $Y_1$ or $Y_2$) depending
on the condition $X_1 < X_2$. That is 
\begin{equation}
 Z = \begin{cases}
   Y_1 & \mbox{if } X_1 < X_2\\
   Y_2 & \mbox{else\,,} 
 \end{cases} \nonumber
\end{equation}
where $X_1, X_2, Y_1$ and $X_1, X_2, Y_2$ are mutually independent random variables. The variables
$Y_1$ and $Y_2$ do not need to be mutually independent. Likewise for the \emph{maximum} and 
\emph{minimum} of random variable introduced above, common terms of random variables are 
removed first. That is
\begin{equation}
 Z = \begin{cases}
   Y_1+C_Y & \mbox{if } X_1+C_X < X_2+C_X\\
   Y_2+C_Y & \mbox{else\,,} 
 \end{cases} \longrightarrow
 Z = C_Y + \begin{cases}
   Y_1 & \mbox{if } X_1 < X_2\\
   Y_2 & \mbox{else\,,} 
 \end{cases}\,.\nonumber
\end{equation}

Given that the possible result variables $Y_1$
and $Y_2$ are independent from the condition, the result variable is then a simple mixture where the weight
is given by the probability of $X_1<X_2$. Hence the density of the result variable $Z$ is
\begin{align}
 f_Z(z) &= Pr[X_1<X_2]\,f_{Y_1}(z) + \left(1-Pr[X_1<X_2]\right)\,f_{Y_2}(z) \nonumber \\
    &= f_{Y_1}(z)\,\int_{-\infty}^{\infty}F_{X_1}(x)\,f_{X_2}(x)\,dx + 
    f_{Y_2}(z)\,\int_{-\infty}^{\infty}F_{X_2}(x)\,f_{X_1}(x)\,dx\,.\nonumber
\end{align}

\subsection{Conditional sum of random variables}
The only non-textbook example of a derived random variable, is the conditional sum of random variables. 
It can be defined as
\begin{equation}
 Z = \begin{cases}
   X_1 + Y_1 & \mbox{if } X_1 < X_2\\
   X_2 + Y_2 & \mbox{else\,,} 
 \end{cases} \nonumber
\end{equation}
where $X_1, X_2, Y_1$ and $X_1, X_2, Y_2$ are mutually independent. $Y_1$ and $Y_2$ may be 
dependent random variables. Although being similar to the conditional random variable, there is an 
important difference: Both possible outcomes ($X_1+Y_1$ and $X_2+Y_2$) 
are not independent from the condition ($X_1+Y_1$ depends trivially on $X_1$).
Therefore, the simple conditional random variable cannot be used here. However, like with the
conditional random variable, common terms are removed first. That is
\begin{equation}
 Z = \begin{cases}
   X_1 + Y_1 + C& \mbox{if } X_1 + C < X_2 + C\\
   X_2 + Y_2 + C& \mbox{else} 
 \end{cases} \longrightarrow 
 Z = C + \begin{cases}
   X_1 + Y_1 & \mbox{if } X_1 < X_2\\
   X_2 + Y_2 & \mbox{else\,.} 
 \end{cases} \nonumber
\end{equation}


The conditional sum of random variables can be used to describe two independent parallel processing
stages where the fastest stage will trigger another stage. For example, if $X_1$ \emph{wins}, it triggers $Y_1$
and if $X_2$ \emph{wins}, it triggers $Y_2$. In contrast to the conditional sum, the 
simple conditional random variable does not trigger a third stage but \emph{selects} the value of a third stage. 
Therefore, the actual waiting time of the \emph{winning} stage does not have an influence on the 
selected one and may lead to non-causal waiting times if $X_1$ and $X_2$ are larger than $Y_1$ and
$Y_2$ (i.e., the response is faster than the processes selecting the response). The conditional sum
of random variables always maintains causality.  	

The conditional	 sum is certainly not common and to my knowledge not
covered in text books. Hence the density for it must be obtained first.
Given that the cases ($X_1<X_2$ and $X_2<X_1$) are mutually exclusive, the density of $Z$, 
$f_Z(z)$ can be expressed as a simple sum. Precisely
\begin{multline}
 f_Z(z) = \iiint_{-\infty}^\infty f(x_1,x_2,y_1|z=x_1+y_1,x_1<x_2)\,dx_1\,dx_2\,dy_1 \\
  + \iiint_{-\infty}^\infty f(x_1,x_2,y_2|z=x_2+y_2,x_2<x_1)\,dx_1\,dx_2\,dy_2\,. \nonumber
\end{multline}

Given that $X_1, X_2$ and $Y_1$ are mutually independent, the first integral can be reduced to
\begin{multline}
\iiint_{-\infty}^\infty f(x_1,x_2,y_1|z=x_1+y_1,x_1<x_2)\,dx_1\,dx_2\,dy_1 \\
  = \iiint_{-\infty}^\infty f_{X_1}(x_1)\,f_{X_2}(x_2)\,f_{Y_1}(y_1)\,\delta(z-x_1-y_1)\,H(x_2-x_1)\,dx_1\,dx_2\,dy_1\\
  = \iint_{-\infty}^\infty f_{X_1}(x_1)\,f_{X_2}(x_2)\,f_{Y_1}(z-x_1)\,H(x_2-x_1)\,dx_1\,dx_2\\
  = \int_{-\infty}^\infty f_{X_1}(x_1)\,f_{Y_1}(z-x_1)\int_{-\infty}^\infty \,f_{X_2}(x_2)\,H(x_2-x_1)\,dx_2\,dx_1\\
  = \int_{-\infty}^\infty f_{X_1}(x_1)\,f_{Y_1}(z-x_1)\int_{x_1}^{\infty} \,f_{X_2}(x_2)\,dx_2\,dx_1\\
  = \int_{-\infty}^\infty f_{X_1}(x_1)\,\left(1-F_{X_2}(x_1)\right)\,f_{Y_1}(z-x_1)\,dx_1\,. \nonumber
\end{multline}

Analogously, the second integral can be reduced to
\begin{multline}
\iiint_{-\infty}^\infty f(z,x_1,x_2,y_2|z=x_2+y_2,x_2<x_1)\,dx_1\,dx_2\,dy_2 \\
 = \int_{-\infty}^\infty \,f_{X_2}(x_2)\left(1-F_{X_1}(x_2)\right)\,f_{Y_2}(z-x_2)\,dx_2\,. \nonumber
\end{multline}

The final density of $Z$ is then given by
\begin{multline}
 f_Z(z) = \int_{-\infty}^\infty f_{X_1}(x_1)\,\left(1-F_{X_2}(x_1)\right)\,f_{Y_1}(z-x_1)\,dx_1\nonumber \\
  + \int_{-\infty}^\infty \,f_{X_2}(x_2)\left(1-F_{X_1}(x_2)\right)\,f_{Y_2}(z-x_2)\,dx_2\,, \nonumber \\
  = \left[f_{X_1}\,\left(1-F_{X_2}\right)\right]\ast f_{Y_1} 
       + \left[f_{X_2}\left(1-F_{X_1}\right)\right]\ast f_{Y_2}\,,\nonumber
\end{multline}
and it can be evaluated using the same \emph{trick} used for chains of random variables.

\subsection{Compound random variables}
The most complex derived random-variable type is the compound random variable. That is a random variable
$X$, distributed according to a parametric distribution $X\sim f_{X|A}(x;A)$ with parameter $A$. $A$, however, 
is itself a random variable with its own distribution $A\sim g(a)$. The distribution of the compound 
random variable $X$ is then obtained by marginalizing the parameter of the PDF $f_{X|A}(\cdot;a)$ as 
\begin{equation}
 f_X(x) = \int f_{X|A}(x;a)\,g(a)\,da\,,\nonumber
\end{equation}
and the CDF is obtained analogously as
\begin{equation}
 F_X(x) = \int F_{X|A}(x;a)\,g(a)\,da\,.\nonumber
\end{equation}

\pkg{SochBB} determines the PDF and CDF of $X$ by performing the following reductions
\begin{align}
 X\sim\delta(x-Y), Y\sim f(y) &\longrightarrow X \sim f(x)\,, \nonumber \\
 X\sim\mathcal{N}(Y,\sigma^2), Y\sim f(y) &\longrightarrow X \sim \mathcal{N}(0, \sigma^2) \ast f(y)\,, \nonumber
\end{align}
or solves the integral numerically.


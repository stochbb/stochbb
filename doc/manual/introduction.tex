 \section{Introduction} \label{sec:intro}
Frequently, complex systems are described in terms of stochastic processes, as the
underlying deterministic process is too complex to be modeled exactly or as the
process is indeed random. It is not always the random process itself that is of 
interest, but a derived quantity. For example, the distribution of waiting times until the
process reaches a certain state. In the field of cognitive psychology, random processes are frequently used to
describe each processing stage in a chain of stages that leads to a response. The state of
each random process itself is usually not measurable but the total response time of all processing 
stages involved. Although each processing stage is modeled as a random process, the waiting-time
of a single stage is just a random variable and the complete system is then a system of dependent random 
variables \cite[e.g.,][]{Reichle2003}.

\pkg{StochBB} is able to describe and analyze complex systems of dependent random variables
by combining simple ones (representing single stages with a known waiting-time distribution) to
a complex system. For example, consider the following independent random variables
\begin{equation}
  X_1 \sim \Gamma(10, 100)\,, X_2 \sim \Gamma(20, 50)\text{ and } X_3 \sim \text{Exp}(0.01)\,,\nonumber
\end{equation}
which are described completely by their distribution. This means that the time, the
processing stage $X_1$ needs to complete is gamma-distributed with shape $k=10$ and scale
$\theta=100$. Analogously, the stages $X_2$ and $X_3$ are defined by their own waiting-time 
distribution.

From these basic building blocks, a more complex system can be assembled by
combining them. Using the example above, one may define a new processing stage that is a chain of
the stages $X_1,\dots,X_3$. This simple chain then describes the successive
processing of information entering the first stage described by $X_1$.
Once the first processing stage finished, its result gets forwarded to the second stage described
by $X_2$ and finally to the last stage described by $X_3$. The waiting time of the
complete chain is again a random variable that is the sum of all random variables,
or expressed mathematically
\begin{equation}
 Y = X_1 + X_2 + X_3\,. \nonumber
\end{equation}

\pkg{SochBB} determines the probability density function (PDF) or cumulative probability function
(CDF) of the waiting-time distribution of the process $Y$ analytically (as far as
possible) or resorts to a numeric method if the analytic approach fails 
\cite[see][for examples of numerical approximations outperforming the stochastic simulation approach]{Thomas2012, Thomas2013}.
More over it provides an efficient and correct sampler for the system of random variables.


\section{Representation and reductions of random variables}
Continuing the example above, please note that the sum of random variables commutes. Hence the
random variable $Y$ remains the same if defined as $Y = X_1 + X_3 + X_2$ instead of 
$Y = X_1 + X_2 + X_3$. Moreover, the 
distribution of the sum of $X_1$ and $X_3$ can be determined analytically as 
$X' = (X_1+X_3)\sim\Gamma(11,100)$. Hence the random variable $Y$ can now be expressed as 
$Y = X_2 + X'$, and only a single numeric convolution is necessary to obtain the PDF of the
random variable $Y$. \pkg{SochBB} implements several of such reductions, exploiting mathematical
identities of random variables and their distributions. To this end, it allows to obtain their PDFs and 
CDFs efficiently. In this section, all basic building blocks that are currently implemented and their 
reductions are presented and discussed in some detail.

\subsection{Affine transformations of random variables}
An affine transformation of the random variable $X$ has the form $Y = a\,X+b$, where $a\neq 0$
and $b$ are real values. Although affine transformations of random variables are not frequently
used directly in a system of random variables, they may appear as a result of other reductions
of the system. The PDF and CDF of the random variable $Y$ defined above, are then 
\begin{equation}
 f_Y(y) = \frac{1}{a}f_X\left(\frac{y-b}{a}\right)\text{ and }
 F_Y(y) = F_X\left(\frac{y-b}{a}\right)\,. \nonumber
\end{equation}

Of course, an affine transformation of an affine transformed random variable $X$ is also a simple 
affine transformation of the random variable. Hence the following reduction is implemented
\begin{equation}
 c\,(a\,X+b)+d \longrightarrow (a\,c)\,X+(c\,b+d)\,.\nonumber
\end{equation}

\subsection{Sums of random variables}
Sums of random variables have been introduced briefly above and may represent a chain of processing
stages being triggered sequentially. The sum itself is a derived random variable that depends on all
mutually independent variables being summed up. The PDF of the sum $Y$ is then the convolution of all
PDFs of the summed variables. That is
\begin{align}
 Y &= \sum_{i=0}^NX_i\text{ where } X_i \sim f_i(x)\text{ mutually independent} \nonumber \\
 Y &\sim f_1(x) \ast \cdots \ast f_N(x)\,. \nonumber
\end{align}

The direct numerical convolution of the underlying distributions can be computationally expensive if the
number of PDFs is large. Assuming that all distributions are well supported on a common interval, however, 
allows for a fast numerical convolution by means of FFT convolution \cite[e.g., ][]{Press2007}. For the FFT convolution, all 
densities $f_i(x)$ are evaluated on a common regular gird and the evaluation of the convolution on that gird
can then be computed easily. This, however, requires that the grid is chosen such that all densities being
convoluted as well as the result are well supported on the chosen interval. Moreover, the grid must be fine 
enough to capture the details of all distributions. 

The numerical convolution, like any numerical approach, is only an approximation. Hence \pkg{SochBB} tries to perform
the convolutions analytically before resorting to the numerical approach. 

First all sums of random variables are flattened. That is, 
\begin{equation}
 \begin{array}{l}
  Y_1 = X_1 + X_2\\
  Y_2 = Y_1 + X_3 
 \end{array} \longrightarrow 
 \begin{array}{l}
  Y_1 = X_1 + X_2\\
  Y_2 = X_1 + X_2 + X_3 
 \end{array}\,, \nonumber
\end{equation}
and all common terms are collected
\begin{equation}
 X_1+X_2+X_1 \longrightarrow 2\,X_1+X_2\,. \nonumber
\end{equation}

Then, the distribution of the sum is derived. Here the following reductions are performed.
\begin{align}
 \delta(x-x_0)\ast f(x) &\longrightarrow f(x-x_0)\,, \nonumber \\
 \phi(x; \mu_1, \sigma_1)\ast \phi(x; \mu_2, \sigma_2) &\longrightarrow 
   \phi(x; \mu_1+\mu_2, \sqrt{\sigma_1^2+\sigma_2^2}) \nonumber\,, \\
 \Gamma(x; k_1, \theta)\ast \Gamma(x; k_2, \theta) &\longrightarrow 
   \Gamma(x; k_1+k_2, \theta)\,, \nonumber
\end{align}
where $\delta(\cdot-x_0)$ is the delta distribution located at $x_0$, $\phi(\cdot; \mu, \sigma)$ 
the normal distribution with mean $\mu$ and standard deviation $\sigma$ and 
$\Gamma(\cdot; k, \theta)$ the gamma distribution with shape $k$ and scale $\theta$.

\subsection{Minimum and maximum of random variables}
The minimum or maximum of some given random variables, e.g. $Y=\min\{X_1,X_2\}$ or 
$Y=\max\{X_1,X_2\}$ are themselves random variables. These derived random variables can be used to
express the waiting time of a processing stage that consists of several independent processes being
performed in parallel (in contrast to sequential processes described by sums of random variables). 
If the complete process finishes once the first of the parallel processes finishes, the 
resulting waiting time can be expressed by the minimum of the underlying random variables. If the process
finishes once all underlying processes are finished, the resulting waiting time can be described
by the maximum.	 

If the two independent random variables $X_1$ and $X_2$ are distributed according to the (cumulative) distribution
functions $F_1(x)$ and $F_2(x)$, the maximum of these two random variables $Y=\max\{X_1,X_2\}$
is distributed according to the probability function $F_Y(y)=F_1(y)\cdot F_2(y)$. Consequently, its PDF
is $f_Y(y)=f_1(y)\cdot F_2(y) + f_2(y)\cdot F_1(y)$, where $f_1(\cdot)$ and $f_2(\cdot)$ are the PDFs
of the random variables $X_1$ and $X_2$. Likewise, the CDF and PDF of the minimum can be expressed as
$F_Y(y)=1-(1-F_1(y))\cdot(1-F_2(y))$ and therefore $f_Y(y) = f_1(y)(1-F_2(y)) + f_2(y)(1-F_1(y))$.

For applying these equations to derive the CDF and PDFs of the minimum and maximum random variables, the 
underlying random variables must be independent. To achieve independence where possible, the following 
reductions are performed on maximum as well as minimum random variables.

Again, first maximum and minimum structures were flattened, for example
\begin{equation}
 \begin{array}{l}
  Y_1 = \max\{X_1,X_2\}\\
  Y_2 = \max\{Y_1,X_3\}
 \end{array} \longrightarrow
 \begin{array}{l}
  Y_1 = \max\{X_1,X_2\}\\
  Y_2 = \max\{X_1,X_2,X_3\}
 \end{array}\,, \nonumber
\end{equation}
then, possible common terms are collected like
\begin{equation}
 \max\{X_1 + X_2, X_3 + X_2\} \longrightarrow \max\{X_1,X_3\}+X_2\,. \nonumber
\end{equation}

The latter transformation does not \emph{ensure} independence of random variables, but 
decreases the complexity of setting-up a complex system of random variables by 
resolving simple-structured dependencies between random variables.

\subsection{Mixture of random variables}
A mixture is the weighted sum of random variables
\begin{equation}
 Y = \frac{w_1X_1+\cdots+w_NX_N}{w_1+\cdots+w_N}\,, \nonumber
\end{equation}
where $w_i$ are positive weights. The result $Y$ is also a random variable with the PDF
\begin{equation}
 f_Y(y) = \frac{w_1f_1(x)+\cdots+w_Nf_N(x)}{w_1+\cdots+w_N}\,,\nonumber
\end{equation}
where $f_i(\cdot)$ is the PDF of the i-th random variable $X_i$. The CDF of the
mixture is obtained analogously. A mixture can be used to describe a random 
path-selection in a system of processing stages. 

\subsection{Conditional random variables}
The \emph{conditional} random variable selects one of two random variables (e.g. $Y_1$ or $Y_2$) depending
on the condition $X_1 < X_2$. That is 
\begin{equation}
 Z = \begin{cases}
   Y_1 & \mbox{if } X_1 < X_2\\
   Y_2 & \mbox{else\,,} 
 \end{cases} \nonumber
\end{equation}
where $X_1, X_2, Y_1$ and $X_1, X_2, Y_2$ are mutually independent random variables. The variables
$Y_1$ and $Y_2$ do not need to be mutually independent. Likewise for the \emph{maximum} and 
\emph{minimum} of random variable introduced above, common terms of random variables are 
removed first. That is
\begin{equation}
 Z = \begin{cases}
   Y_1+C_Y & \mbox{if } X_1+C_X < X_2+C_X\\
   Y_2+C_Y & \mbox{else\,,} 
 \end{cases} \longrightarrow
 Z = C_Y + \begin{cases}
   Y_1 & \mbox{if } X_1 < X_2\\
   Y_2 & \mbox{else\,,} 
 \end{cases}\,.\nonumber
\end{equation}

Given that the possible result variables $Y_1$
and $Y_2$ are independent from the condition, the result variable is then a simple mixture where the weight
is given by the probability of $X_1<X_2$. Hence the density of the result variable $Z$ is
\begin{align}
 f_Z(z) &= Pr[X_1<X_2]\,f_{Y_1}(z) + \left(1-Pr[X_1<X_2]\right)\,f_{Y_2}(z) \nonumber \\
    &= f_{Y_1}(z)\,\int_{-\infty}^{\infty}F_{X_1}(x)\,f_{X_2}(x)\,dx + 
    f_{Y_2}(z)\,\int_{-\infty}^{\infty}F_{X_2}(x)\,f_{X_1}(x)\,dx\,.\nonumber
\end{align}

\subsection{Conditional sum of random variables}
One of the few non-textbook examples of a derived random variable is the conditional sum of random variables. 
It can be defined as
\begin{equation}
 Z = \begin{cases}
   X_1 + Y_1 & \mbox{if } X_1 < X_2\\
   X_2 + Y_2 & \mbox{else\,,} 
 \end{cases} \nonumber
\end{equation}
where $X_1, X_2, Y_1$ and $X_1, X_2, Y_2$ are mutually independent. $Y_1$ and $Y_2$ may be 
dependent random variables. Although being similar to the conditional random variable, there is an 
important difference: Both possible outcomes ($X_1+Y_1$ and $X_2+Y_2$) 
are not independent from the condition (i.e. $X_1+Y_1$ depends trivially on $X_1$).
Therefore, the simple conditional random variable cannot be used here. However, likewise the
conditional random variable, common terms are removed first. That is
\begin{equation}
 Z = \begin{cases}
   X_1 + Y_1 + C& \mbox{if } X_1 + C < X_2 + C\\
   X_2 + Y_2 + C& \mbox{else\,,} 
 \end{cases} \longrightarrow 
 Z = C + \begin{cases}
   X_1 + Y_1 & \mbox{if } X_1 < X_2\\
   X_2 + Y_2 & \mbox{else\,,} 
 \end{cases} \nonumber
\end{equation}


The conditional sum of random variables can be used to describe two independent parallel processing
stages where the fastest stage will trigger another stage. For example, if $X_1$ \emph{wins}, it triggers $Y_1$
and if $X_2$ \emph{wins} it triggers $Y_2$. In contrast to the conditional sum, the 
simple conditional random variable does not trigger a third stage but \emph{selects} the value of a third stage. 
Therefore, the actual waiting time of the \emph{winning} stage does not have an influence on the 
selected one and may lead to non-causal waiting times if $X_1$ and $X_2$ are larger than $Y_1$ and
$Y_2$ (i.e., the response is faster than the processes selecting the response). The conditional sum
of random variables always maintains causality.  	

The conditional	 sum is certainly not common and to my knowledge not
covered in text books. Hence the density for it must be obtained first.
Given that the cases ($X_1<X_2$ and $X_2<X_1$) are mutually exclusive, the density of $Z$, 
$f_Z(z)$ can be expressed as a simple sum. Precisely
\begin{multline}
 f_Z(z) = \iiint_{-\infty}^\infty f(x_1,x_2,y_1|z=x_1+y_1,x_1<x_2)\,dx_1\,dx_2\,dy_1 \\
  + \iiint_{-\infty}^\infty f(x_1,x_2,y_2|z=x_2+y_2,x_2<x_1)\,dx_1\,dx_2\,dy_2\,. \nonumber
\end{multline}

Given that $X_1, X_2$ and $Y_1$ are mutually independent, the first integral can be reduced to
\begin{multline}
\iiint_{-\infty}^\infty f(x_1,x_2,y_1|z=x_1+y_1,x_1<x_2)\,dx_1\,dx_2\,dy_1 \\
  = \iiint_{-\infty}^\infty f_{X_1}(x_1)\,f_{X_2}(x_2)\,f_{Y_1}(y_1)\,\delta(z-x_1-y_1)\,H(x_2-x_1)\,dx_1\,dx_2\,dy_1\\
  = \iint_{-\infty}^\infty f_{X_1}(x_1)\,f_{X_2}(x_2)\,f_{Y_1}(z-x_1)\,H(x_2-x_1)\,dx_1\,dx_2\\
  = \int_{-\infty}^\infty f_{X_1}(x_1)\,f_{Y_1}(z-x_1)\int_{-\infty}^\infty \,f_{X_2}(x_2)\,H(x_2-x_1)\,dx_2\,dx_1\\
  = \int_{-\infty}^\infty f_{X_1}(x_1)\,f_{Y_1}(z-x_1)\int_{x_1}^{\infty} \,f_{X_2}(x_2)\,dx_2\,dx_1\\
  = \int_{-\infty}^\infty f_{X_1}(x_1)\,\left(1-F_{X_2}(x_1)\right)\,f_{Y_1}(z-x_1)\,dx_1\,. \nonumber
\end{multline}

Analogously, the second integral can be reduced to
\begin{multline}
\iiint_{-\infty}^\infty f(z,x_1,x_2,y_2|z=x_2+y_2,x_2<x_1)\,dx_1\,dx_2\,dy_2 \\
 = \int_{-\infty}^\infty \,f_{X_2}(x_2)\left(1-F_{X_1}(x_2)\right)\,f_{Y_2}(z-x_2)\,dx_2\,. \nonumber
\end{multline}

The final density of $Z$ is then given by
\begin{multline}
 f_Z(z) = \int_{-\infty}^\infty f_{X_1}(x_1)\,\left(1-F_{X_2}(x_1)\right)\,f_{Y_1}(z-x_1)\,dx_1\nonumber \\
  + \int_{-\infty}^\infty \,f_{X_2}(x_2)\left(1-F_{X_1}(x_2)\right)\,f_{Y_2}(z-x_2)\,dx_2\,, \nonumber \\
  = \left[f_{X_1}\,\left(1-F_{X_2}\right)\right]\ast f_{Y_1} 
       + \left[f_{X_2}\left(1-F_{X_1}\right)\right]\ast f_{Y_2}\,,\nonumber
\end{multline}
and it can be evaluated using the same \emph{trick} used for chains of random variables.

\subsection{Compound random variables}
The most complex derived random-variable type is the compound random variable. That is a random variable
$X$ distributed according to a parametric distribution $X\sim f_{X|A}(x;A)$ with parameter $A$. $A$, however, 
is itself a random variable with its own distribution $A\sim g(a)$. The distribution of the compound 
random variable $X$ is then obtained by marginalizing the parameter of the PDF $f_{X|A}(\cdot;a)$ as 
\begin{equation}
 f_X(x) = \int f_{X|A}(x;a)\,g(a)\,da\,,\nonumber
\end{equation}
and the CDF is obtained analogously as
\begin{equation}
 F_X(x) = \int F_{X|A}(x;a)\,g(a)\,da\,.\nonumber
\end{equation}

\pkg{SochBB} determines the PDF and CDF of $X$ by performing the following reductions
\begin{align}
 X\sim\delta(x-Y), Y\sim f(y) &\longrightarrow X \sim f(x)\,, \nonumber \\
 X\sim\mathcal{N}(Y,\sigma^2), Y\sim f(y) &\longrightarrow X \sim \mathcal{N}(0, \sigma^2) \ast f(y)\,, \nonumber
\end{align}
or solves the integral numerically.

